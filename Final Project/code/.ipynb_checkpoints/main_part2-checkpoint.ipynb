{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "177fa2ff-3702-4d1a-8aa1-df19459b383d",
   "metadata": {},
   "source": [
    "# Main For Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a0ab8-216c-4075-b637-be9c58f94c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Needed for training\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "# Models\n",
    "from models_part1 import FinalClassifier\n",
    "from models_part2 import MNISTClassifyingAutoencoder,CIFAR10ClassifyingAutoencoder\n",
    "from models_testing import  plot_accuracies,plot_losses,test_classifier,test_classifyingAutoEncoder\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a1c16-cc77-4f1f-b367-e3ce9280b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2fe48-3488-40af-a4ad-a954c1cf7135",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900feca7-b0d4-4935-95d3-e40228f4039d",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ba0c8-4189-47a6-9198-c5244eebc778",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = \"./mnist_data\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=mnist_path,  \n",
    "    train=True,       \n",
    "    transform=transform,  # Apply transformations here\n",
    "    download=True     \n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=mnist_path,\n",
    "    train=False,  \n",
    "    transform=transform,  # Apply same transformations for test data\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(\"MNIST dataset downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d708f-ee65-41c4-8084-bc2c2e46a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 50_000\n",
    "val_size = 10_000\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders\n",
    "mnist_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 1)\n",
    "mnist_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers = 1)\n",
    "mnist_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 1)\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n",
    "\n",
    "images, labels = next(iter(mnist_train_loader))\n",
    "print(f\"Batch shape: {images.shape}, Labels: {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763c5e3-9f6e-42de-bf9e-3b0af9cb37a4",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633fe9a-3fe7-4591-a001-40892a7bc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = MNISTClassifyingAutoencoder(latent_dim=128).to(device)\n",
    "\n",
    "train_losses, train_accuracies, val_accuracies = model.train_autoencoder(train_loader= mnist_train_loader,\n",
    "                       val_loader=mnist_val_loader,\n",
    "                       num_epochs=15,\n",
    "                       learning_rate=1e-3,\n",
    "                       weight_decay= 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7cc31-dd52-4aaa-84dd-d9efe51e96e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses = train_losses, val_losses = val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659a4e8-a358-4a94-ba7d-6d669fdba9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_encoder = model.encoder\n",
    "for param in pretrained_encoder.parameters():\n",
    "    param.requires_grad = False  # Ensure encoder is frozen\n",
    "classifier = FinalClassifier(latent_dim=128)\n",
    "train_losses, train_accuracies, val_accuracies = classifier.fit_classifier(encoder = pretrained_encoder,\n",
    "                                                                           train_loader =  mnist_train_loader,\n",
    "                                                                           val_loader = mnist_val_loader,\n",
    "                                                                           num_epochs=15, \n",
    "                                                                           learning_rate=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec650238-83c0-476b-be54-368dee367e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses)\n",
    "plot_accuracies(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a01ba59-c92b-4bd5-bdc5-2fa348d265e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(encoder=pretrained_encoder,\n",
    "                classifier=classifier,\n",
    "                test_loader=mnist_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7c1fb5-201e-45b5-8728-6575720b4245",
   "metadata": {},
   "source": [
    "# Cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e3676-5884-42e5-84a6-ba8602072e92",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250b51a-259d-4bbf-8d61-786ee67849d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_path = \"./cifar10_data\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "c10_full_train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=cifar10_path,  \n",
    "    train=True,       \n",
    "    transform=transform,\n",
    "    download=True     \n",
    ")\n",
    "\n",
    "c10_test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=cifar10_path,\n",
    "    train=False,  \n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(\"CIFAR-10 dataset downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137abcaf-c60f-4bde-84b5-30ecc59975b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 40_000\n",
    "val_size = 10_000\n",
    "\n",
    "c10_train_dataset, c10_val_dataset = random_split(c10_full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Check dataset sizes\n",
    "print(f\"Train size: {len(c10_train_dataset)}, Validation size: {len(c10_val_dataset)}, Test size: {len(c10_test_dataset)}\")\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders\n",
    "c10_train_loader = DataLoader(c10_train_dataset, batch_size=batch_size, shuffle=True, num_workers = 1)\n",
    "c10_val_loader = DataLoader(c10_val_dataset, batch_size=batch_size, shuffle=False, num_workers = 1)\n",
    "c10_test_loader = DataLoader(c10_test_dataset, batch_size=batch_size, shuffle=False, num_workers = 1)\n",
    "\n",
    "print(\"DataLoaders created successfully!\")\n",
    "\n",
    "# Get a batch of images and labels from the train_loader\n",
    "images, labels = next(iter(c10_train_loader))\n",
    "print(f\"Batch shape: {images.shape}, Labels (first 5): {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79af0c7-4a11-4699-9e73-4ef1b6f03d0e",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa26697-788e-4487-900a-919fb88d7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = CIFAR10ClassifyingAutoencoder(latent_dim=128).to(device)\n",
    "\n",
    "train_losses, train_accuracies, val_accuracies = model.train_autoencoder(train_loader= c10_train_loader,\n",
    "                                                                         val_loader=c10_val_loader,\n",
    "                                                                         num_epochs=25,\n",
    "                                                                         learning_rate=1e-3,\n",
    "                                                                         weight_decay= 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4175be8-1cba-40c5-a50d-192b5f5e77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses = train_losses, val_losses = val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe135c0d-911a-4ebe-8f58-8ff39571f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False  # Ensure encoder is frozen\n",
    "pretrained_encoder = model.encoder\n",
    "classifier = FinalClassifier(latent_dim=128)\n",
    "train_losses, train_accuracies, val_accuracies = classifier.fit_classifier(encoder = pretrained_encoder,\n",
    "                                                                           train_loader =  c10_train_loader,\n",
    "                                                                           val_loader = c10_val_loader,\n",
    "                                                                           num_epochs=30, \n",
    "                                                                           learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c20dcc-3ae9-4c15-8712-cc747ed3149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses)\n",
    "plot_accuracies(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccbfbe-ebe5-4cec-98aa-ba57b66240c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classifier(encoder=pretrained_encoder,\n",
    "                classifier=classifier,\n",
    "                test_loader=c10_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
