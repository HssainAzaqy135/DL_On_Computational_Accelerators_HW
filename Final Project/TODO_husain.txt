Done:

- Separate train,val and test sets [V]
- Report train accuracy in train loops too [V]
- Add epoch timing [V]
- Try leakyRelu [V]
- Try new reduced Architecture [V]
- Try removing the last activation from the cifar encoder [V]

- 1.2.2 for cifar10 [V]
- separate models, plotting and testing to different files [V]

- Try MOE encoder [V, not better , way slower]

- part 3 implementation [V]
- try high batch sizes like 128 , 256 ,512[V]


Todo:
 - fix cifar10 part3 convergence []
	- try other modefiers like color jitter[V]
	- try grad clipping [V]

Important:!!!!!!!!!!!!11
- apply all clippings of augmentations to mnist and cifar10 together []
